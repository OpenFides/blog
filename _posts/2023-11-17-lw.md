---
layout: post
title: lw
---


 
摘要
本文是根据质数以及互质数特性，再结合初中学习的正比例函数，一次函数等理论知识，探索质数在实际生活中的应用。
首先根据基础的数学理论，结合计算机信息以及Python编程技术，经过假设、推导、实验、总结和应用等一步步从基础的理论探求质数在数据比较，数据加密以及数据同步等方面的实际应用。接着根据本次的探索，使用Python编写完成了两类数据摘要算法和一个对称加密算法，最后结合实际数据同步应用场景给出了本次创新探索过程中得到的滚动式数据同步程序实现。
关键词
名词	解释
二进制	计算机中使用0或1表现数据，一般以8个位作为一个基于处理单元。称之为比特（byte）
质数	又叫素数，一个大于1且只能被1和它本身整除的自然数。
互质	若N个整数的最大公因数是1，则称这N个整数互质。
散列	也叫哈希或摘要，是将任意长度输入，转成固定长度（一般比输入长度小）输出算法，它具有单向性的特点。
正比例函数	一般两个变量x、y之间的关系式可以表示成形如y=kx的函数（k为常数，x的次数为1，且k≠0），那么y=kx就叫做正比例函数。正比例函数属于一次函数，它是一次函数的一种特殊形式。
即一次函数形如：y=kx+b（k为常数，且k≠0）中，当b=0时，即所谓“y轴上的截距”为零，则叫做正比例函数。
滚动摘要	计算摘要时可以根据按数据流顺序计算固定长度的摘要。除首次计算外，后一次的计算都利用了前一次计算的结果，从而提供了大量摘要的计算速度。
论文正文
引言
我们在小学就已经学习到质数、合数以及互质等概念。
质数也叫素数，英文为：prime numbers，是一个大于1且只能被1和它本身整除的自然数。关于质数还有一个有趣的猜想 - 哥德巴赫猜想 - 任一大于2的整数都可写成三个质数之和。后面又发展为神秘的1+1=2的证明问题。即任一大于2的偶数都可写成两个质数之和。
不过，数学的世界好像离我们的现实生活很远。素数除了它“唯二”的特性，就是深奥且无法证明的猜想。直到初中学习正比例函数，我发现：质数其实在现实生活中已经无处不在，也如它英文中的prime（首要的）一样prime。本文将通过探索质数在现实生活的应用，给出几个创新算法和基于这些算法的数据同步方案。
散列
假设
在学习正比例函数时，我们已经知道它是一次函数（y=kx+b）的一种特殊形式。通过数形结合，我们也可以把它理解成几何中的直线，把k理解为斜率，把b理解为偏移。
那么，如果把k设为一个质数，并将这个函数限定在一个范围中，形式如下：
y=(kx+b)%M
其中M代表范围，那么y是否会随x变化在M范围内不规则地出现呢？
编码
基于上面的设想，使用python编写一段代码，如下：
def hash_test(k):
    y = 1
    for v in range(0, 256):
        y = (k * y + v) & 255  # 求256的除数可以直接使用位的与运算
        print(y)
其中k参数作为实验的输入参数，为了方便实验数据的收集，这里将M设置在一个字节的大小（即256）。偏移b分别从0到255作为测试数据。
实验
我们分别使用三个质数（3，7，37）和三个合数（4，8，32）作为斜率k的值运行上面的函数。
实验数据见附件，利用Excel生成的图表如下：
 
图表 1质数作为斜率
 
图表 2合数作为斜率
结论
如图表1、2所示：
	当使用质数作为斜率时，y随x的变化波动比较离散 - 不规则。
	当使用合数作为斜率时，y随x的变化波动比较规则有周期性。
应用
这种离散的特性可以快速判定两个数据内容不同。比如要比较两段文本是否相同，可以先快速计算出这两段文本散列值，然后直接比较这两个散列值，如果散列值不同则这两个文件肯定内容不同，如果散列值相同，则这两个文件可能内容不同。
以下为Python实现的代码和测试结果：
def hash(data):
    y = 1
    for v in data:
        y = (31 * y + v) & 0xffff # 这里限制在int32大小
    return y

print(hash("哪怕一点点的差异hash值也会相差很大".encode()))  #789928
print(hash("哪怕一点点的差异hash值也会相差很大!".encode()))#7710585
从执行的结果可以看出：输入的一段文本哪怕只有一点差异也会体现不很在的不同。
3.4小节将详细介绍散列算法在数据同步中的创新应用。
加密
假设
通常人们想到的数据加密，可以在明文的基础上进行适当的偏移。比如：凯撒密码（Caesar），它在加密时会将明文中的每个字母都按照其在字母表中的顺序向后或向前移动固定数目作为密文。例如，当偏移量为3时，明文的每个字符向左移3个字母，解密时将密文的每个字母向右移3个字母。这里的密钥就是3：
比如明文：ABC对应的密文为：DEF。显然这种加密强度最多只要尝试25次即可得到明文。
有了上面的散列算法的实验，是否可以利用这种离散不规律的特性，生产更复杂安全的加密方法呢？
这里将原来hash函数稍微修改一下：
f(X)=(AX+B)%M
其中X可以视作初始种子，乘数A、增量B选取一些特殊值。这里假定可以通过用户指定的密钥按一定的规则生成。
因为在计算机中通常以一个Byte（2的8次方）作为基本数据存储单位，所以模数M = 256即可。
编码
基于上面的设想，使用python编写一段代码，如下：
def createCipher_Test(X: int, A: int, B: int):
    result = []
    for v in range(0, 256):
        X = (A * X + B) & 255  # 求256的除数可以直接使用位的与运算
        result.append(X)
    return result
这段代码与求hash值不同之处在于：X，A，B的值都不固定。需要根据质数与互质的特性，找到一种组合生成足够随机离散的数列。
实验
这次实验并没有想象中简单，因为涉及到多个参数，需要进行大量实验与推导。
首先，从上面的数学函数可以分析出： X作初始种子对数列的随机性不会产生影响。所以实验时我们直接将它设置为了。
然后，通过嵌套循环尝试A，B可能的取值。为了能得到一定的规则，同时也不至少生成太多的数据。本次实验假定A，B∈[3,32]做了少量数据采集。详见附件中的实验数据（密码表规则.csv）。
实验使用的Python脚本如下：
data = {}
s = 0
for a in range(3, 33):
    for b in range(3, 33):
        data[str(a) + "<>"+str(b)] = createCipher_Test(s, a, b)

df = pd.DataFrame(data=data)
df.to_csv('密码表规则.csv')
接着通过Excel打开csv文件找出所有数据去重后行没有减少的列。部分实验数据分析界面如下：
 
图表 3密码表规则分析

结论
经过上面大量数据的分析，在生成安全密码本时，乘数A、增量B和模数M必须满足以下条件：
	B与M互质；
	M的所有质因数都能整除A-1；
	A-1必须是4的倍数；
同时，于是实验数据只小范围采样。为了实际应用少出意外Bug，本探索中额外做了一些限制：
	A，B必须是正整数且小于M的256。
应用
加密时首先创建一个密码本，可以由用户输入一段文本作为密钥。这里规定将文本的hash值作为种子。再用文本的长度作为生成A和B的参数。具体实现的逻辑见下面的Python代码：
def createCipherDict(key: str):
    X = hash(key.encode())     # 取密钥的hash值作为种子
    A = (len(key) % 16)*4 + 1  # 保证 A-1是4的倍数且小于256
    B = len(key) & 255  
    # B与M互质
    if B<3:
        B = 3
    while math.gcd(B,256)!=1:
        B-=1
    encodeDict = bytearray(256)
    decodeDict = bytearray(256)
    for i in range(0, 256):
        X = (A * X + B) & 255  # 求256的除数可以直接使用位的与运算
        encodeDict[i] = X  
        decodeDict[X] = i 
    return encodeDict, decodeDict
通过密钥创建好的密码本后，就可以快速通过查密码本对明文加密或对密文解密，加解密的代码实现如下：
def encode(dict, data):
    d = bytearray()
    i = 0
    for v in data:
        d.append(dict[v])
    return d  

def decode(dict, data):
    d = bytearray()
    i = 0
    for v in data:
        d.append(dict[v])
    return d 
以下是调用加解密码算法的代码和运行的结果：
encodeDict, decodeDict = createCipherDict("my private key") 
plaintext = "ABC"
ciphertext = encode(encodeDict,plaintext.encode())
plaintext = decode(decodeDict,  ciphertext)
print(base64.b64encode(ciphertext).decode()) #kVYz 
print(plaintext.decode())                    #ABC
通过上面的实验，利用质数与互质特性创建的加密算法比只通过偏移进行加密的算法，比如：凯撒密码，要安全可靠很多。
数据同步
说明
数据同步是现代计算机科学与信息化管理中十分重要且必不可少的应用。前面探索质数特性，在数据比较和加密算法中的应用与实践，其目标就是要完成一个快速、安全且高效的数据同步解决方案。
在排除硬件限制，快速主要体现在：从大量数据中快速比较出相同或不同，即找差异。前面提到的的散列（也叫哈希）算法具体快速计算数据摘要并具有以下特性：
	可以100%否定 – 如果两个哈希值不同则它们本身肯定不同（这也是质数的特性）
	不能100%肯定 – 如果两个哈希值相同则它们本身可能相同，也可能不同。存在摘要碰撞的可能性。
安全，在内容的保密方面可以很自然地想到加密。利用前面提到的安全密码本进行加解密，速度也是可以接收。字典本身是O(0)计算复杂度。在内容的防丢失，防串改方面，同样可以使用本文的散列算法进行内容确认。
高效，当前涉及到不同设备间数据同步（现实数据同步往往也是发生在不同设备之上）就需要比较所有数据，并找到数据源和目标源的差异。如果这个比较过程需要传输所有数据，显然是不经济与高效的。
传统的数据同步，主要是基于目标数据或数据块与源数据或数据块逐一摘要比较然后再同步（传输差异数据）。
这里存在一个重要的事实：现实的数据往往不是单一的增加。比如我们写的作文，当发现作文写得不好，我们往往是在写得不好的地方直接修改，并不会在作文的最后增加修改的内容。
接下来，将探索了一个更具创新的滚动比较算法，以便实现一个快速、安全且高效的数据同步解决方案。
流程
滚动式数据同步具体流程如下：
 
图表 4数据同步的根本流程
在数据比较上，将使用两种不同的摘要算法 – 弱摘要和强摘要。前者用于快速100%肯定不同。后者用于近100%肯定相同（直接使用成熟的MD5算法）。
这里创新的点是：在比较数据差异时使用滚动偏移一个字节，从而尽可能精确到出所有不同与相同数据块，从而达到高效的目标。
不过，从上面的流程可知：尽管我们使用了弱摘要算法，但是当需要同步的数据量很大时，依然会涉及到大量的摘要计算。因此我们需要设置一种更高效的摘要算法。
滚动弱摘要
滚动弱摘要，顾名思义就是可以在源数据上“滚动”式持续快速计算数据块的摘要。算法设计如下：
假设有一段长度为n数据D，则：simple_hash(D)=B×65536+A。
simple_hash的结果是一个int32正整数，它由两个部分组成，其中：
后半部分A=1+∑_(i=1)^n▒(D_i )  mod 65521 
前半部分B=n+∑_(i=1)^n▒(D_1+D_2+...+D_i )  mod 65521
以下是Python的代码实现：
def simple_hash(data: bytes):
    A = 1
    B = 0
    for i in range(len(data)):
        A += data[i]
        B += A
    D = B * (2**16) + (A % 65521)
    return A, B, D

def rolling_simple_hash(A: int, B, oldbyte: int, newbyte: int, length: int):
    A = A - oldbyte + newbyte
    B = B - length * oldbyte + A - 1
    D = (B << 16) + (A % 65521)
    return A, B, D

因为要实现“滚动”计算，这里代码分成了两个函数。同时，由于Python语言特点，数学公式中乘法和求余数，我也使用几种不同的方式，不过最终功能一致。
注：65521 是最接近65536（2的16次方）的质数。
应用
用Python代码实现如下：
# 需要数据同步的信息
source_path = Path("source.txt")
target_path = Path("target.txt")
target_dict = {}
blob_size = 128
data = []
start = 0
# 读取目标文件的弱/强摘要
with target_path.open('rb') as target_file:
    data = target_file.read(blob_size)
    while len(data) > 0:
        A, B, D = simple_hash(data=data)
        d = target_dict.get(D, None)
        if d == None:
            target_dict[D] = {hashlib.md5(data).hexdigest()}
        else:
            d.add(hashlib.md5(data).hexdigest())
        data = target_file.read(blob_size)

# print(str(target_dict))
# 读取源文件数据
with source_path.open('rb') as source_file:
    data = source_file.read()
# 找出源文件数据与目标数据的相同块
same_list = []
start = 0
end = 0
if (start+blob_size < len(data)):
    end = start + blob_size
else:
    end = len(data)
A, B, D = simple_hash(data=data[start:end])
while True:
    d = target_dict.get(D, None)
    # 如果数据块相同
    if d != None and hashlib.md5(data[start:end]).hexdigest() in d:
        same_list.append({"start": start, "end": end, "data": data[start:end]})
        if (end+blob_size <= len(data)):
            start += blob_size
            end += blob_size
        else:
            start = end
            end = len(data)
        if start >= len(data):
            break
        A, B, D = simple_hash(data=data[start:end])
    else:
        start += 1
        end += 1
        if end > len(data):
            end = len(data)
        if start >= len(data):
            break

        A, B, D = rolling_simple_hash(
            A, B, data[start-1], data[end-1], blob_size)
# print(same_list)

# 合并相同与不同的数据块
merge_path = Path("merge.txt")
with merge_path.open('wb') as merge_file:
    if (len(same_list) == 0):
        merge_file.write(data)
    else:
        start = 0
        for i in range(0, len(same_list)):
            if start < same_list[i]["start"]:
                x = data[start: same_list[i]["start"]]
                print("新文件内容\r\n" + x.decode())
                merge_file.write(x)
                start = same_list[i]["start"]
            if start == same_list[i]["start"]:
                x = same_list[i]["data"]
                print("原文件内容\r\n" + x.decode())
                merge_file.write(x)
                start = same_list[i]["end"]
        if start < len(data):
            x = data[start:len(data)]
            print("新文件内容\r\n" + x.decode())
            merge_file.write(x)
根据以上的代码实现，运行附件中示例文件，可以看到：数据同步时，只有修改部分的一小段内容会被传输到目标文件中。
此示例中同步的新文件内容只有以下部分：
新文件内容
he spirit back to dust.
Whether 60 or 16, there is in every human being’s heart the lure of wonders, the unfailing appetite f
总结
不足
由于个人的时间与精力有限，在代码实现上未进行充分测试。最终的作品（滚动数据同步方案）也仅是提供论证的必要部分。作品本身具有一点研究和参考价值，但还不足够用于实际项目工程或工作中去。
在探索质数的实际应用的过程中，走了很多弯路。在生成密码本的理论推导中使用了“蛮力”统计。但实际结论可能并不十分准确（也可能缺在错误结论）。
创新点
在探索质数的实际应用的过程中，将一次函数与质数结合是我一个意外的想法。并且后来发现一次函数还能用于生成密码本也只是为了学习信息技术和Python编程而做的尝试。
由于缺少理论基础，在走了很多弯路后，原本以为假设可能会是失败 – 因为之前也有过很多假设失败的例子 – 没想到的是生成的密码本随机性还是令人满意。
另外，在设计数据同步时，本想着只要按数据顺序比较散列值而非数据本身即可。但父亲提醒我：数据同步实际场景往往都是在原内容中增加或减少数据，所以只按数据从上而下按顺序分块比较散列值，将会有大量重复的数据被不必要的传输。这倒逼了我去思路如何尽量避免重复数据传输。因此，想到了滚动计算和比较。这其实也打破了我之前设计的散列算法。最终在交稿前几天想到了滚动弱摘要rolling_simple_hash（详见3.4.3）
感想与体会
通过本次探索质数的实际应用，我深刻体会到了数学的美妙之处，质数的概念简单，但是它却有着丰富的性质与应用。这种简单而有趣的性质，让我感受到了数学的魅力和智慧。在探索的过程中，我遇到了许多看似简单又有趣的问题。对于这些问题，看似简单，但需要灵活运用数学知识和思维方法来解答。通过不断思考与实践，我逐渐提高了自己的逻辑推理能力和问题解决能力。学数学最重要的就是要善于思考。如果把数学比作一把锁的话，那思考就是一把开锁的金钥匙，为你打开这数学之锁。我们要学习蜜蜂那样的工作方法，既会采蜜，又会酿蜜。数学是利用学过的知识来解决未知的问题。学习数学要有毅力、有耐心、有恒心。同时，在学习数学的同时，一定要学习思考，学会灵活运用，举一反三，这样才能取得事半功倍的好成绩。最后，还有一个深刻体会，学习与研究往往需要坚持，只要坚持不懈才可能终见曙光。




